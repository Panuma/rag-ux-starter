# -*- coding: utf-8 -*-
import os
import re
import json
from pathlib import Path
from datetime import datetime

import streamlit as st
import chromadb
from dotenv import load_dotenv

# =========================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ —É—Ç–∏–ª–∏—Ç—ã
# =========================
load_dotenv()

DB_DIR = "./storage/chroma"
OFFLINE_ONLY = os.getenv("OFFLINE_ONLY", "true").lower() == "true"
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL", "gpt-4o")

BASE_DIR = Path(__file__).resolve().parent
ASSETS_DIR = (BASE_DIR / "03_assets").resolve()
TEXTS_DIR  = (BASE_DIR / "02_clean_texts").resolve()

# ---------- –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º ----------
def resolve_image_path(p: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—É—Ç—å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
    if not p:
        return ""
    p = str(p).strip()
    if p.startswith("http://") or p.startswith("https://"):
        return p
    if re.match(r"^[A-Za-z]:\\", p):  # –∞–±—Å–æ–ª—é—Ç–Ω—ã–π Windows-–ø—É—Ç—å
        return p
    rel = p.lstrip("/\\").replace("\\", "/")
    candidates = [
        BASE_DIR / rel,
        ASSETS_DIR / rel,
        ASSETS_DIR / Path(rel).name,
        BASE_DIR / "03_assets" / "/".join(rel.split("/")[1:])
    ]
    for c in candidates:
        if c.exists():
            return str(c)
    return str(candidates[0])

def _append_img(imgs, item):
    if isinstance(item, str):
        imgs.append((item, None))
    elif isinstance(item, dict):
        path = item.get("path") or item.get("src") or item.get("url")
        if path:
            imgs.append((path, item.get("alt")))

def _parse_images(images_raw):
    """meta['images'] ‚Üí —Å–ø–∏—Å–æ–∫ –ø–∞—Ä (path, alt). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç list/dict/JSON/str."""
    imgs = []
    if images_raw is None:
        return imgs
    if isinstance(images_raw, list):
        for it in images_raw: _append_img(imgs, it)
    elif isinstance(images_raw, dict):
        _append_img(imgs, images_raw)
    elif isinstance(images_raw, str):
        try:
            parsed = json.loads(images_raw)
            if isinstance(parsed, list):
                for it in parsed: _append_img(imgs, it)
            elif isinstance(parsed, dict):
                _append_img(imgs, parsed)
            else:
                _append_img(imgs, images_raw)
        except Exception:
            _append_img(imgs, images_raw)
    return imgs

# ---------- —Ñ–æ–ª–±—ç–∫: –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ MD ----------
IMG_PATTERN = re.compile(r"!\[[^\]]*\]\(([^)]+)\)")
HDR_RE      = re.compile(r"^#{1,6}\s+.+?$", flags=re.MULTILINE)

def _slice_section(md_text: str, section_path: str) -> str:
    if not md_text or not section_path:
        return md_text or ""
    i = md_text.lower().find(section_path.strip().lower())
    if i == -1:
        return md_text
    tail = md_text[i:]
    m = HDR_RE.search(tail, pos=1)  # —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ ¬´—Ö–≤–æ—Å—Ç–µ¬ª
    return tail[:m.start()] if m and m.start() > 0 else tail

def _fallback_images_from_md(meta, max_count=3):
    """–ï—Å–ª–∏ meta['images'] –ø—É—Å—Ç–æ, –±–µ—Ä—ë–º 2‚Äì3 –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ —Å–µ–∫—Ü–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ .md."""
    filename = (meta or {}).get("filename")
    if not filename:
        return []
    md_path = TEXTS_DIR / filename
    if not md_path.exists():
        return []
    # —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    try:
        text = md_path.read_text(encoding="utf-8")
    except Exception:
        try:
            text = md_path.read_text(encoding="utf-16")
        except Exception:
            return []
    section_path = (meta or {}).get("section_path") or ""
    slice_text = _slice_section(text, section_path)
    found = IMG_PATTERN.findall(slice_text) or IMG_PATTERN.findall(text)
    found = [p.strip().split("?")[0] for p in found][:max_count]
    return [(p, None) for p in found]

# ---------- –æ—Ñ–ª–∞–π–Ω-—Å–≤–æ–¥ (—ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ) ----------
def offline_summary(top_hits, max_sent=4):
    """–ü—Ä–æ—Å—Ç–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–≤–æ–¥: –≤—ã–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ top-—Ö–∏—Ç–æ–≤."""
    import heapq, re as _re
    text = " ".join((h.get("text") or "") for h in top_hits[:8])
    sents = _re.split(r"(?<=[.!?])\s+", text)
    sents = [s.strip() for s in sents if s.strip()]
    if len(sents) <= max_sent:
        return " ".join(sents)
    def score(s):
        sc = len(s)
        for kw in ("–ø—Ä–æ—Ü–µ–Ω—Ç", "–ª–∏–º–∏—Ç", "–∑–∞–¥–æ–ª–∂", "–±–µ—Å–ø—Ä–æ—Ü–µ–Ω—Ç", "–∫–æ–º–∏—Å", "—Å–Ω—è—Ç–∏", "–ø–µ—Ä–µ–≤–æ–¥", "–ø–æ–Ω—è—Ç", "—ç–∫—Ä–∞–Ω", "–≤–∏–∑—É–∞–ª"):
            if kw in s.lower(): sc += 30
        return sc
    best = heapq.nlargest(max_sent, sents, key=score)
    return " ".join(best)

# ---------- OFFLINE –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: TF-IDF + TextRank (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫) ----------
def _tokenize(text):
    text = text.lower()
    # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
    text = re.sub(r"[^a-z–∞-—è0-9—ë\- ]+", " ", text)
    # —Ä—É—Å—Å–∫–∏–µ/–∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä)
    stop = set("""
        –∏ –≤ –≤–æ –Ω–µ —á—Ç–æ –æ–Ω –Ω–∞ —è —Å —Å–æ –∫–∞–∫ –∞ —Ç–æ –≤—Å–µ –æ–Ω–∞ —Ç–∞–∫ –µ–≥–æ –Ω–æ –¥–∞ —Ç—ã –∫ —É –∂–µ –≤—ã –∑–∞ –±—ã –ø–æ —Ç–æ–ª—å–∫–æ –µ–µ –º–Ω–µ –±—ã–ª–æ
        –≤–æ—Ç –æ—Ç –º–µ–Ω—è –µ—â–µ –Ω–µ—Ç –æ –∏–∑ –ª–∏ –∂–µ –¥–æ –Ω–∏ –∫—Ç–æ —ç—Ç–æ —Ç–æ–≥–æ –ø–æ—Ç–æ–º—É —ç—Ç–æ—Ç –∫–∞–∫–æ–π –≥–¥–µ –∫–æ–≥–¥–∞ –∑–¥–µ—Å—å —Ç–∞–º –Ω–µ–≥–æ –Ω–µ–µ –Ω–∏—Ö
        –ø—Ä–∏ —á–µ–º —Ä–∞–∑ –¥–≤–∞ —Ç—Ä–∏ –∏–ª–∏ –ª–∏–±–æ —Ç–∞–∫–∂–µ —Ç–∞–∫–∂–µ-—Ç–æ —á—Ç–æ–±—ã –¥–ª—è –±–µ–∑ –ø—Ä–æ –Ω–∞–¥ –ø–æ–¥ –º–µ–∂–¥—É –æ–∫–æ–ª–æ
        the a an to of is are was were be been being this that these those and or not from by as with into onto about
    """.split())
    toks = [t for t in text.split() if t and t not in stop and len(t) > 2]
    return toks

def _sent_split(text):
    parts = re.split(r"(?<=[.!?])\s+", text)
    return [p.strip() for p in parts if len(p.strip()) > 0]

def _build_tfidf(sentences):
    # —Å–ª–æ–≤–∞—Ä—å
    docs = [ _tokenize(s) for s in sentences ]
    vocab = {}
    for d in docs:
        for t in d:
            if t not in vocab:
                vocab[t] = len(vocab)
    # tf
    tf = []
    for d in docs:
        vec = [0]*len(vocab)
        for t in d:
            vec[vocab[t]] += 1
        tf.append(vec)
    # idf
    import math
    df = [0]*len(vocab)
    for j in range(len(vocab)):
        for i in range(len(docs)):
            if tf[i][j] > 0:
                df[j] += 1
    idf = [ math.log((1+len(docs))/(1+dfj))+1.0 for dfj in df ]
    # tf-idf –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞
    tfidf = []
    for i in range(len(docs)):
        row = [ tf[i][j]*idf[j] for j in range(len(vocab)) ]
        # l2
        norm = math.sqrt(sum(x*x for x in row)) or 1.0
        row = [x/norm for x in row]
        tfidf.append(row)
    return tfidf

def _cosine(v1, v2):
    s = 0.0
    for a,b in zip(v1,v2):
        s += a*b
    return s

def _textrank_scores(tfidf, damping=0.85, iters=30, eps=1e-6):
    n = len(tfidf)
    if n == 0:
        return []
    # –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–±–µ–∑ –¥–∏–∞–≥–æ–Ω–∞–ª–∏)
    sim = [[0.0]*n for _ in range(n)]
    for i in range(n):
        for j in range(i+1,n):
            c = _cosine(tfidf[i], tfidf[j])
            sim[i][j] = c
            sim[j][i] = c
    # –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
    row_sum = [sum(sim[i]) for i in range(n)]
    M = [[ (sim[i][j]/row_sum[i] if row_sum[i] > 0 else 0.0) for j in range(n)] for i in range(n)]
    # PageRank
    pr = [1.0/n]*n
    for _ in range(iters):
        new = [ (1.0-damping)/n + damping * sum(pr[j]*M[j][i] for j in range(n)) for i in range(n) ]
        # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        diff = sum(abs(new[i]-pr[i]) for i in range(n))
        pr = new
        if diff < eps:
            break
    return pr

def summarize_tfidf_textrank(hits, query:str, max_sentences:int=5):
    """–ë–µ—Ä—ë–º —Ç–æ–ø-—Ö–∏—Ç—ã ‚Üí —Ä–µ–∂–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Üí —Å—á–∏—Ç–∞–µ–º TF-IDF –∏ TextRank ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–≤–æ–¥."""
    text = " ".join((h.get("text") or "") for h in hits[:10])
    # –ø–æ–¥–º–µ—à–∞–µ–º —Å–∞–º –∑–∞–ø—Ä–æ—Å (–¥–∞—ë—Ç –ª—ë–≥–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å–ª–æ–≤–∞–º –∏–∑ –∑–∞–ø—Ä–æ—Å–∞)
    text = (query or "") + ". " + text
    sentences = _sent_split(text)
    # —Ñ–∏–ª—å—Ç—Ä —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö
    sentences = [s for s in sentences if len(_tokenize(s)) >= 5]
    if not sentences:
        return ""
    tfidf = _build_tfidf(sentences)
    ranks = _textrank_scores(tfidf)
    # –±–µ—Ä—ë–º top-N –ø–æ —Ä–∞–Ω–≥—É, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ—è–≤–ª–µ–Ω–∏—è
    idx_sorted = sorted(range(len(sentences)), key=lambda i: ranks[i], reverse=True)[:max_sentences]
    idx_sorted = sorted(idx_sorted)
    # –ª—ë–≥–∫–∞—è ¬´—Å–∫–ª–µ–π–∫–∞¬ª: —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞—á–∞–ª—É
    result = []
    seen = set()
    for i in idx_sorted:
        s = sentences[i].strip()
        key = s[:40].lower()
        if key not in seen:
            result.append(s)
            seen.add(key)
    return " ".join(result)

# =========================
# UI: –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ + —Å—Ç–∏–ª–∏
# =========================
st.set_page_config(page_title="–ò–ò-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å 2.0", page_icon="üîé", layout="wide")
st.markdown("""
<style>
.card {border:1px solid #e6e6e6; border-radius:14px; padding:14px 16px; margin:12px 0; background:#fff;}
.card h4 {margin:0 0 6px 0; font-size:16px;}
.badges {display:flex; flex-wrap:wrap; gap:6px; margin-bottom:8px;}
.badge {font-size:12px; background:#f5f5f7; border:1px solid #eee; padding:2px 8px; border-radius:999px; color:#333;}
.quote {font-size:14px; line-height:1.55; margin:8px 0 12px; padding-left:10px; border-left:3px solid #e3e3e3; color:#111;}
.meta {font-size:12px; color:#666;}
</style>
""", unsafe_allow_html=True)
st.title("–ò–ò-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å 2.0 ‚Äî –ø–æ–∏—Å–∫ –∏ –æ—Å–º—ã—Å–ª–µ–Ω–∏–µ UX-–∫–µ–π—Å–æ–≤")

# =========================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
# =========================
st.sidebar.markdown("## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

# –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
use_offline_mode = st.sidebar.toggle(
    "üîÑ –†–µ–∂–∏–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏", 
    value=OFFLINE_ONLY,
    help="–û—Ñ–ª–∞–π–Ω: –ª–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (TF-IDF + TextRank)\n–û–Ω–ª–∞–π–Ω: ChatGPT + –ª–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤"
)

# –í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω–¥–∏–∫–∞—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ä–µ–∂–∏–º–∞
if use_offline_mode:
    st.sidebar.success("üü¢ –û—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º")
else:
    st.sidebar.info("üîµ –û–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º")

# debug_images = st.sidebar.toggle("üîß –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–ª–∞–¥–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", value=False)

# –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Ä–µ–∂–∏–º–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
if use_offline_mode:
    st.info("üü¢ **–û—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º**: –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é TF-IDF –∏ TextRank")
else:
    st.info("üîµ **–û–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ChatGPT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤")

# =========================
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º Chroma
# =========================
client = chromadb.PersistentClient(path=DB_DIR)
text_collection = client.get_or_create_collection("ux_research")   # —á–∞–Ω–∫-—Ç–µ–∫—Å—Ç—ã

# =========================
# TABs: –ü–æ —Ç–µ–∫—Å—Ç—É / –ü–æ –º–∞–∫–µ—Ç—É
# =========================
tab_text, tab_image = st.tabs(["üîé –ü–æ —Ç–µ–∫—Å—Ç—É", "üñº –ü–æ –º–∞–∫–µ—Ç—É"])

with tab_text:
    # --- –§–∏–ª—å—Ç—Ä—ã –∏ –≤–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞ ---
    col1, col2, col3 = st.columns(3)
    iteration = col1.text_input("–ò—Ç–µ—Ä–∞—Ü–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1 –∏–ª–∏ 2)", "")
    scenario  = col2.text_input("–°—Ü–µ–Ω–∞—Ä–∏–π/—Ç–µ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–±–µ—Å–ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥')", "")
    date_hint = col3.text_input("–î–∞—Ç–∞ (YYYY-MM, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", "")
    query     = st.text_area("–ò–¥–µ—è —ç–∫—Ä–∞–Ω–∞ / –≤–æ–ø—Ä–æ—Å / –≥–∏–ø–æ—Ç–µ–∑–∞", "–ö–∞–∫ –ø–æ–∫–∞–∑–∞—Ç—å –±–µ—Å–ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞–ª–∏ —Å –ø–ª–∞–Ω–æ–º –≤—ã–ø–ª–∞—Ç?")

    # --- –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É ---
    def search_text(q: str, k: int = 12):
        res = text_collection.query(query_texts=[q], n_results=k)
        hits = []
        for i in range(len(res["documents"][0])):
            hits.append({
                "text": res["documents"][0][i],
                "meta": res["metadatas"][0][i],
                "dist": res["distances"][0][i] if "distances" in res else None
            })
        return hits

    def format_cite(hit, idx):
        md = hit["meta"] or {}
        label = f"{md.get('id')} ¬∑ {md.get('title')} ¬∑ –∏—Ç–µ—Ä–∞—Ü–∏—è {md.get('iteration')} ¬∑ {md.get('date')}"
        where = f"{md.get('filename')} / {md.get('section_path') or '‚Ä¶'} / chunk {md.get('chunk_index')}"
        return f"[{idx}] {label}\n{where}\n\"{(hit.get('text') or '').strip()}\""

    def llm_answer(q, top_hits, offline_mode):
        if offline_mode:
            return ("**–ò—Ç–æ–≥ (–æ—Ñ–ª–∞–π–Ω).** –ù–∏–∂–µ ‚Äî –ª—É—á—à–∏–µ —Ü–∏—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É.\n\n" +
                    "\n\n".join(format_cite(h, i+1) for i, h in enumerate(top_hits[:3])))
        try:
            from openai import OpenAI
            client_oai = OpenAI()
            context = "\n\n".join(format_cite(h, i+1) for i, h in enumerate(top_hits[:5]))
            prompt = (
                "–¢—ã ‚Äî UX-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n"
                "1) –ò—Ç–æ–≥ (2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).\n"
                "2) 1‚Äì3 –¥–æ—Å–ª–æ–≤–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã (—Å—Ç–∞–≤—å [–Ω–æ–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞]).\n"
                "3) –ò—Å—Ç–æ—á–Ω–∏–∫–∏: id ¬∑ title ¬∑ iteration ¬∑ date ¬∑ filename/section/chunk.\n\n"
                f"–í–æ–ø—Ä–æ—Å: {q}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n"
            )
            resp = client_oai.chat.completions.create(
                model=OPENAI_LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            return resp.choices[0].message.content
        except Exception as e:
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–∑–≤–∞—Ç—å LLM ({e}). –ü–æ–∫–∞–∑–∞–Ω—ã —Ü–∏—Ç–∞—Ç—ã:\n\n" + \
                   "\n\n".join(format_cite(h, i+1) for i, h in enumerate(top_hits[:3]))

    # ================ –ö–Ω–æ–ø–∫–∞ –ø–æ–∏—Å–∫–∞ ================
    if st.button("–ò—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∫–µ–π—Å—ã", type="primary"):
        with st.spinner("–ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã..."):
            hits = search_text(query, k=12)

            # –§–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            def meta_ok(md):
                ok = True
                if iteration and (md.get("iteration", "") != iteration): ok = False
                if date_hint and (md.get("date", "").strip() != date_hint.strip()): ok = False
                if scenario and scenario.lower() not in (md.get("tags", "") or "").lower(): ok = False
                return ok

            filtered = [h for h in hits if meta_ok(h["meta"])] or hits

            # ======================= –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ =======================
            st.markdown("## üß† –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥")

            if use_offline_mode:
                # –æ—Ñ–ª–∞–π–Ω-¬´–≥–µ–Ω–µ—Ä–∞—Ü–∏—è¬ª: TF-IDF + TextRank
                summary_text = summarize_tfidf_textrank(filtered, query, max_sentences=5)
            else:
                # –µ—Å–ª–∏ –æ–Ω–ª–∞–π–Ω ‚Äî —Å–Ω–∞—á–∞–ª–∞ LLM, –∞ –Ω–∏–∂–µ –æ—Ñ–ª–∞–π–Ω –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤
                llm_out = llm_answer(query, filtered, use_offline_mode)
                st.markdown(llm_out)
                summary_text = offline_summary(filtered, max_sent=4)

            # –∫—Ä–∞—Å–∏–≤–æ –≤—ã–≤–µ–¥–µ–º –æ—Ñ–ª–∞–π–Ω-–≤—ã–≤–æ–¥ –∫–∞–∫ bullets
            if summary_text:
                bullets = [s.strip() for s in re.split(r"(?<=[.!?])\s+", summary_text) if len(s.strip()) > 0][:4]
                st.write("- " + "\n- ".join(bullets))
            else:
                st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–≤–æ–¥–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å.")

            # ======================= –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞ =======================
            st.markdown("### üíæ –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞")
            # –°—Ñ–æ—Ä–º–∏—Ä—É–µ–º markdown-–æ—Ç—á—ë—Ç
            def build_md_report():
                now = datetime.now().strftime("%Y-%m-%d %H:%M")
                parts = []
                parts.append(f"# –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç ‚Äî –ò–ò-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å 2.0\n")
                parts.append(f"**–í—Ä–µ–º—è:** {now}\n")
                parts.append(f"**–ó–∞–ø—Ä–æ—Å:** {query}\n")
                parts.append("## –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥\n")
                if summary_text:
                    for b in bullets:
                        parts.append(f"- {b}")
                    parts.append("")
                else:
                    parts.append("_–ù–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–≤–æ–¥–∞._\n")
                parts.append("## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ü–∏—Ç–∞—Ç—ã\n")
                for i, h in enumerate(filtered[:8], start=1):
                    md = h["meta"] or {}
                    label = f"{md.get('id')} ¬∑ {md.get('title')} ¬∑ –∏—Ç–µ—Ä–∞—Ü–∏—è {md.get('iteration')} ¬∑ {md.get('date')}"
                    where = f"{md.get('filename')} / {md.get('section_path') or '‚Ä¶'} / chunk {md.get('chunk_index')}"
                    parts.append(f"### [{i}] {label}")
                    parts.append(where)
                    parts.append("")
                    parts.append(f"> { (h.get('text') or '').strip() }")
                    # –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    parsed_imgs = _parse_images(md.get("images") or [])
                    if not parsed_imgs:
                        parsed_imgs = _fallback_images_from_md(md, max_count=3)
                    if parsed_imgs:
                        parts.append("")
                        parts.append("–°–≤—è–∑–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
                        for p, _ in parsed_imgs[:3]:
                            parts.append(f"![img]({p})")
                        parts.append("")
                return "\n".join(parts)

            md_report = build_md_report()
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .md",
                data=md_report.encode("utf-8"),
                file_name="ux_insights_report.md",
                mime="text/markdown"
            )

            # –ü–æ–ø—ã—Ç–∫–∞ PDF (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω reportlab)
            try:
                from reportlab.lib.pagesizes import A4
                from reportlab.pdfgen import canvas
                from reportlab.lib.units import mm

                def save_pdf(text: str, path: Path):
                    c = canvas.Canvas(str(path), pagesize=A4)
                    width, height = A4
                    x, y = 20*mm, height - 20*mm
                    for line in text.splitlines():
                        # –ø—Ä–æ—Å—Ç–µ–π—à–∞—è –æ–±—Ä–µ–∑–∫–∞ –ø–æ —à–∏—Ä–∏–Ω–µ
                        while len(line) > 110:
                            c.drawString(x, y, line[:110])
                            line = line[110:]
                            y -= 6*mm
                            if y < 15*mm:
                                c.showPage(); y = height - 20*mm
                        c.drawString(x, y, line)
                        y -= 6*mm
                        if y < 15*mm:
                            c.showPage(); y = height - 20*mm
                    c.save()

                pdf_path = BASE_DIR / "ux_insights_report.pdf"
                # –¥–ª—è PDF –≤–æ–∑—å–º—ë–º —Ç–µ–∫—Å—Ç –±–µ–∑ markdown-–∫–∞—Ä—Ç–∏–Ω–æ–∫ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)
                text_for_pdf = re.sub(r"!\[[^\]]*\]\([^)]+\)", "[image]", md_report)
                save_pdf(text_for_pdf, pdf_path)

                with open(pdf_path, "rb") as f:
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å .pdf",
                        data=f.read(),
                        file_name="ux_insights_report.pdf",
                        mime="application/pdf"
                    )
            except Exception:
                st.caption("–°–æ–≤–µ—Ç: –µ—Å–ª–∏ –Ω–µ—Ç PDF –≤ –æ–¥–∏–Ω –∫–ª–∏–∫ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä –∫–∞–∫ PDF (Print ‚Üí Save as PDF).")

            # ======================= –ë—ã—Å—Ç—Ä—ã–π –æ—Ñ–ª–∞–π–Ω-—Å–≤–æ–¥ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è =======================
            if use_offline_mode:
                st.markdown("### üß© –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –æ—Ñ–ª–∞–π–Ω-—Å–≤–æ–¥ (—ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω—ã–π)")
                st.write(offline_summary(filtered, max_sent=4))

            st.divider()
            st.subheader("üìå –¢–æ–ø-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã")

            # ======================= –†–µ–Ω–¥–µ—Ä –∫–∞—Ä—Ç–æ—á–µ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ =======================
            for i, h in enumerate(filtered[:6], start=1):
                md = h["meta"] or {}
                title = md.get("title") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                badges = [
                    f'id: {md.get("id") or "‚Äî"}',
                    f'–∏—Ç–µ—Ä–∞—Ü–∏—è: {md.get("iteration") or "‚Äî"}',
                    f'–¥–∞—Ç–∞: {md.get("date") or "‚Äî"}',
                    f'—Ñ–∞–π–ª: {md.get("filename") or "‚Äî"}',
                ]
                section = md.get("section_path") or "‚Ä¶"
                chunk_ix = md.get("chunk_index") or "‚Äî"

                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.markdown(f'<h4>[{i}] {title}</h4>', unsafe_allow_html=True)
                st.markdown('<div class="badges">' + "".join([f'<div class="badge">{b}</div>' for b in badges]) + '</div>', unsafe_allow_html=True)
                st.markdown(f'<div class="quote">{(h.get("text") or "").strip()}</div>', unsafe_allow_html=True)
                st.markdown(f'<div class="meta">{md.get("product") or ""} ¬∑ —Å–µ–∫—Ü–∏—è: {section} ¬∑ chunk: {chunk_ix}</div>', unsafe_allow_html=True)

                # ----- –ü—Ä–µ–≤—å—é: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö -----
                parsed_imgs = _parse_images(md.get("images") or [])

                # ----- –ï—Å–ª–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç–æ ‚Äî —Ñ–æ–ª–±—ç–∫ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ Markdown -----
                if not parsed_imgs:
                    parsed_imgs = _fallback_images_from_md(md, max_count=3)

                # ----- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–µ–π –∏ –ø–æ–∫–∞–∑ -----
                resolved = []
                for p, alt in parsed_imgs:
                    rp = resolve_image_path(p)
                    exists = (not rp.startswith("http")) and Path(rp).exists()
                    resolved.append({"path": rp, "alt": alt, "exists": True if rp.startswith("http") else exists})
                to_show = [r for r in resolved if r["exists"]][:3]
                if to_show:
                    cols = st.columns(len(to_show))
                    for j, info in enumerate(to_show):
                        with cols[j]:
                            st.image(info["path"], caption=info["alt"] or Path(info["path"]).name, use_container_width=True)
                elif parsed_imgs:
                    st.caption("‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã, –Ω–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç–∏.")

                # if st.sidebar.toggle("üîß –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç–ª–∞–¥–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", value=False, key=f"dbg_{i}"):
                #     with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º", expanded=False):
                #         st.write(resolved)

                st.markdown('</div>', unsafe_allow_html=True)

with tab_image:
    st.subheader("–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ (CLIP)")
    uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PNG/JPG –º–∞–∫–µ—Ç–∞", type=["png","jpg","jpeg","webp"])
    if uploaded:
        from PIL import Image
        from sentence_transformers import SentenceTransformer
        image = Image.open(uploaded).convert("RGB")
        st.image(image, caption="–ó–∞–ø—Ä–æ—Å", use_container_width=True)

        # —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (CLIP)
        clip_model = SentenceTransformer("clip-ViT-B-32")
        qvec = clip_model.encode([image], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()

        # –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ux_images
        client_img = chromadb.PersistentClient(path=DB_DIR)
        images_coll = client_img.get_or_create_collection("ux_images")
        res = images_coll.query(query_embeddings=[qvec], n_results=12, include=["metadatas","documents","distances"])

        hits = []
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –ª—é–±–æ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø–æ–ª—è
        result_count = len(res["metadatas"][0]) if "metadatas" in res and res["metadatas"] else 0
        
        for i in range(result_count):
            md = res["metadatas"][0][i] or {}
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ ID, –µ—Å–ª–∏ ids –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            item_id = res["ids"][0][i] if "ids" in res and res["ids"] else f"item_{i}"
            hits.append({
                "id": item_id,
                "path": md.get("path"),
                "name": md.get("filename"),
                "dist": res["distances"][0][i] if "distances" in res and res["distances"] else None
            })

        if hits:
            st.markdown("#### –ü–æ—Ö–æ–∂–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è")
            cols = st.columns(4)
            for i, h in enumerate(hits):
                with cols[i % 4]:
                    st.image(h["path"], caption=f"{h['name']}", use_container_width=True)
        else:
            st.info("–ù–∏—á–µ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å. –£–±–µ–¥–∏—Å—å, —á—Ç–æ –∏–Ω–¥–µ–∫—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–±—Ä–∞–Ω (`build_images_index.py`).")
